{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w2_ Seq2seq Class Code.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishnarevi/TSAI_END2.0_Session7/blob/main/QnA_dataset_seq2seq_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ8mZ2kNcgpn"
      },
      "source": [
        "#  **Neural Machine Translation with Q/A Dataset**\n",
        "Here we will be building a sequence to sequence deep learning model using PyTorch and TorchText for prediction of answers for given questions.Sequence to Sequence (seq2seq) model here uses an encoder-decoder architecture. Encoder neural network encodes the input sequence(question) into a single vector, also called as a Context Vector,which is an abstract representation of the input sequence.This vector is then passed into the decoder neural network, which is used to output the corresponding output sequence (answer), one word at a time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHfZozWApLPO"
      },
      "source": [
        "## Preparing Data\n",
        "Import all the required modules and  set the random seeds for deterministic results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d27S03zbAAPu"
      },
      "source": [
        "#importing necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import torch, torchtext\n",
        "from torchtext import legacy\n",
        "from torchtext.legacy import data\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "#setting  seed\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tDxAcumpYn1"
      },
      "source": [
        "Mount drive to access dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7l29xRJWqhf",
        "outputId": "7c6bee3c-6b45-4f67-e587-afe0fac0a079"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQSoI0keXGLf"
      },
      "source": [
        "path='/content/drive/MyDrive/TSAI_data/QA_pairs/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oakmHfQXpddd"
      },
      "source": [
        "###Dataset \n",
        "\n",
        "We use Question/Answer dataset generated by students \n",
        "who took undergraduate natural language processing courses taught by Noah Smith \n",
        "at Carnegie Mellon and Rebecca Hwa at the University of Pittsburgh during \n",
        "Spring 2008, Spring 2009, and Spring 2010.\n",
        "\n",
        "There are three files, one for each year of students: S08, S09, and S10.\n",
        "\n",
        "Each file contains the questions and answers. The first line of the file contains \n",
        "column names for the tab-separated data fields in the file.\n",
        "Let's merge all three files and create our final dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "Qh5Azc50XigI",
        "outputId": "6d8edadd-4277-4f86-fd83-da75322d1eff"
      },
      "source": [
        "file_1=pd.read_csv(path+\"question_answer_pairs_s08.txt\",sep='\\t',encoding='ISO-8859-1')\n",
        "file_2=pd.read_csv(path+\"question_answer_pairs_s09.txt\",sep='\\t',encoding='ISO-8859-1')\n",
        "file_3=pd.read_csv(path+\"question_answer_pairs_s10.txt\",sep='\\t',encoding='ISO-8859-1')\n",
        "files=[file_1,file_2,file_3]\n",
        "df=pd.concat(files,axis=0)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ArticleTitle</th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>DifficultyFromQuestioner</th>\n",
              "      <th>DifficultyFromAnswerer</th>\n",
              "      <th>ArticleFile</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Abraham_Lincoln</td>\n",
              "      <td>Was Abraham Lincoln the sixteenth President of...</td>\n",
              "      <td>yes</td>\n",
              "      <td>easy</td>\n",
              "      <td>easy</td>\n",
              "      <td>data/set3/a4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Abraham_Lincoln</td>\n",
              "      <td>Was Abraham Lincoln the sixteenth President of...</td>\n",
              "      <td>Yes.</td>\n",
              "      <td>easy</td>\n",
              "      <td>easy</td>\n",
              "      <td>data/set3/a4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Abraham_Lincoln</td>\n",
              "      <td>Did Lincoln sign the National Banking Act of 1...</td>\n",
              "      <td>yes</td>\n",
              "      <td>easy</td>\n",
              "      <td>medium</td>\n",
              "      <td>data/set3/a4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Abraham_Lincoln</td>\n",
              "      <td>Did Lincoln sign the National Banking Act of 1...</td>\n",
              "      <td>Yes.</td>\n",
              "      <td>easy</td>\n",
              "      <td>easy</td>\n",
              "      <td>data/set3/a4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Abraham_Lincoln</td>\n",
              "      <td>Did his mother die of pneumonia?</td>\n",
              "      <td>no</td>\n",
              "      <td>easy</td>\n",
              "      <td>medium</td>\n",
              "      <td>data/set3/a4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      ArticleTitle  ...   ArticleFile\n",
              "0  Abraham_Lincoln  ...  data/set3/a4\n",
              "1  Abraham_Lincoln  ...  data/set3/a4\n",
              "2  Abraham_Lincoln  ...  data/set3/a4\n",
              "3  Abraham_Lincoln  ...  data/set3/a4\n",
              "4  Abraham_Lincoln  ...  data/set3/a4\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SjXoB2lqZeO"
      },
      "source": [
        "Let's view length of final dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1emdowWaLfk",
        "outputId": "aad81c65-b4f7-45e8-8c2b-5b36526a4741"
      },
      "source": [
        "print(f'Length of dataset \\n{len(df)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of dataset \n",
            "3998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-se5-6GsQF3"
      },
      "source": [
        "Next we will do some preprocessing , will remove all unwanted columns for training and check if any nan rows are there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "2ef_Hj8QbuD9",
        "outputId": "67035e7b-5512-41ce-88e0-43ec49e0a35e"
      },
      "source": [
        "\n",
        "df=df.iloc[:,1:3]\n",
        "df=df.dropna().reset_index(drop=True)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Was Abraham Lincoln the sixteenth President of...</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Was Abraham Lincoln the sixteenth President of...</td>\n",
              "      <td>Yes.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Did Lincoln sign the National Banking Act of 1...</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Did Lincoln sign the National Banking Act of 1...</td>\n",
              "      <td>Yes.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Did his mother die of pneumonia?</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3417</th>\n",
              "      <td>What areas do the Grevy's Zebras inhabit?</td>\n",
              "      <td>semi-arid grasslands of Ethiopia and northern ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3418</th>\n",
              "      <td>Which species of zebra is known as the common ...</td>\n",
              "      <td>Plains Zebra (Equus quagga, formerly Equus bur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3419</th>\n",
              "      <td>Which species of zebra is known as the common ...</td>\n",
              "      <td>Plains Zebra</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3420</th>\n",
              "      <td>At what age can a zebra breed?</td>\n",
              "      <td>five or six</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3421</th>\n",
              "      <td>At what age can a zebra breed?</td>\n",
              "      <td>5 or 6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3422 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Question                                             Answer\n",
              "0     Was Abraham Lincoln the sixteenth President of...                                                yes\n",
              "1     Was Abraham Lincoln the sixteenth President of...                                               Yes.\n",
              "2     Did Lincoln sign the National Banking Act of 1...                                                yes\n",
              "3     Did Lincoln sign the National Banking Act of 1...                                               Yes.\n",
              "4                      Did his mother die of pneumonia?                                                 no\n",
              "...                                                 ...                                                ...\n",
              "3417          What areas do the Grevy's Zebras inhabit?  semi-arid grasslands of Ethiopia and northern ...\n",
              "3418  Which species of zebra is known as the common ...  Plains Zebra (Equus quagga, formerly Equus bur...\n",
              "3419  Which species of zebra is known as the common ...                                       Plains Zebra\n",
              "3420                     At what age can a zebra breed?                                        five or six\n",
              "3421                     At what age can a zebra breed?                                             5 or 6\n",
              "\n",
              "[3422 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CCIWc-bsdF9"
      },
      "source": [
        "Let's split dataset into train and test for modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyifaw8oNgLo"
      },
      "source": [
        "\n",
        "(df_train, df_test) = train_test_split(df, test_size=0.3, random_state=random.seed(SEED))\n",
        "\n",
        "assert len(df_train) + len(df_test) == len(df)\n",
        "df_train=df_train.reset_index(drop=True)\n",
        "df_test=df_test.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOxw1cFKNtqU",
        "outputId": "7c16e650-6ade-4c8e-f7b0-006a1dad842c"
      },
      "source": [
        "len(df_train),len(df_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2395, 1027)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGS-Zzs2Aglb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc038212-f1d2-41a7-e4b2-4c56bf453306"
      },
      "source": [
        "%%bash\n",
        "\n",
        "python -m spacy download en --quiet\n",
        "# python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4op_sItqmH-"
      },
      "source": [
        "Load spacy model for tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKK9oA7OArZK"
      },
      "source": [
        "# spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrgnBIPArB0o"
      },
      "source": [
        "We create the tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPubajj7A0pY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b54c644a-cf1d-47cd-ca6c-41b238761644"
      },
      "source": [
        "# def tokenize_an(text):\n",
        "#     \"\"\"\n",
        "#     Tokenizes text from a string into a list of strings (tokens) and reverses it\n",
        "#     \"\"\"\n",
        "#     return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "def tokenize_qn(text):\n",
        "    \"\"\"\n",
        "    Tokenizes text from a string into a list of strings (tokens)\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "### Sample Run ###\n",
        "\n",
        "sample_text = \"I love machine learning\"\n",
        "print(tokenize_qn(sample_text))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'love', 'machine', 'learning']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OJzfltsf4Be"
      },
      "source": [
        "Here our source (SRC - Input) is `Question` and target (TRG - Output) is `Answer`. We also add 2 extra tokens \"start of sequence\"`<sos>` and \"end of sequence\"`<eos>` for effective model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztK5PjShBN_M"
      },
      "source": [
        "SRC = Field(tokenize = tokenize_qn, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_qn, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg31nBV3sjS2"
      },
      "source": [
        "Define fields to decide how we process data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR61PPo7NJOo"
      },
      "source": [
        "fields=[('Question',SRC),('Answer',TRG)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "7Mb5bnPlOx--",
        "outputId": "25c1bd72-7290-4af9-a822-81ab7b2f3755"
      },
      "source": [
        "df_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Does a violin have four strings?</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Is Uruguay located in the northwesten part of ...</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Did Charles-Augustin de Coulomb come from a po...</td>\n",
              "      <td>No, his mother came from a wealthy family in t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What are Carleton University's athletic teams ...</td>\n",
              "      <td>Carleton Ravens</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Why do kangaroos have a wide bite?</td>\n",
              "      <td>Because of grazing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2390</th>\n",
              "      <td>Why does the cornet have a slightly mellower t...</td>\n",
              "      <td>The cornet and flugelhorn have conical bores</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2391</th>\n",
              "      <td>Does the Portuguese language have its roots in...</td>\n",
              "      <td>Yes, Portuguese is derived from Latin.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2392</th>\n",
              "      <td>What are turtle eggs covered in when they incu...</td>\n",
              "      <td>mud or sand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2393</th>\n",
              "      <td>In what ways was Adams opposed by Anderw Hamil...</td>\n",
              "      <td>Hamilton wanted to control the army differentl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2394</th>\n",
              "      <td>Is he widely considered to be one of the great...</td>\n",
              "      <td>Yes.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2395 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Question                                             Answer\n",
              "0                      Does a violin have four strings?                                                Yes\n",
              "1     Is Uruguay located in the northwesten part of ...                                                 no\n",
              "2     Did Charles-Augustin de Coulomb come from a po...  No, his mother came from a wealthy family in t...\n",
              "3     What are Carleton University's athletic teams ...                                    Carleton Ravens\n",
              "4                    Why do kangaroos have a wide bite?                                 Because of grazing\n",
              "...                                                 ...                                                ...\n",
              "2390  Why does the cornet have a slightly mellower t...       The cornet and flugelhorn have conical bores\n",
              "2391  Does the Portuguese language have its roots in...             Yes, Portuguese is derived from Latin.\n",
              "2392  What are turtle eggs covered in when they incu...                                        mud or sand\n",
              "2393  In what ways was Adams opposed by Anderw Hamil...  Hamilton wanted to control the army differentl...\n",
              "2394  Is he widely considered to be one of the great...                                               Yes.\n",
              "\n",
              "[2395 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83hU-jxyOy1X"
      },
      "source": [
        "example_train=[data.Example.fromlist([df_train.Question[i],df_train.Answer[i]],fields) for i in range (df_train.shape[0])]\n",
        "example_test=[data.Example.fromlist([df_test.Question[i],df_test.Answer[i]],fields) for i in range (df_test.shape[0])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3rnMrons20U"
      },
      "source": [
        "Now we create train and test dataset from list of rows "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jEQDQMtP9Lx"
      },
      "source": [
        "train_data = torchtext.legacy.data.Dataset(example_train, fields)\n",
        "test_data= torchtext.legacy.data.Dataset(example_test,fields)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3cCTrkoB7fu",
        "outputId": "a465538c-8a8a-4bb6-af05-ba4063eaf816"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 2395\n",
            "Number of testing examples: 1027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkcjHbbuQP5V"
      },
      "source": [
        "\n",
        "Let's look at one of the examples in the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN6IJggTCBH6",
        "outputId": "af3f2d18-eb09-42bb-8e1b-5e9e76b6eb6e"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Question': ['does', 'a', 'violin', 'have', 'four', 'strings', '?'], 'Answer': ['yes']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4Ngo4bzresG"
      },
      "source": [
        "Build vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY3qVJbpK_2L"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy0xQVgdLBkm",
        "outputId": "b6795cf9-fb47-4981-c3ec-27c008d28b22"
      },
      "source": [
        "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in source (de) vocabulary: 2011\n",
            "Unique tokens in target (en) vocabulary: 1271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EErELxiqrgXy"
      },
      "source": [
        "Define device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I_59ly4LC4T"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoftkP1Xrij-"
      },
      "source": [
        "Create iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0GNSQSCLEOB"
      },
      "source": [
        "BATCH_SIZE=32\n",
        "train_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data,  test_data), \n",
        "    batch_size = BATCH_SIZE,sort_within_batch = True,\n",
        "    sort_key = lambda x: len(x.Question),\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPtdS37GjhX8"
      },
      "source": [
        "### Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6kktU14kk0i"
      },
      "source": [
        " #### Encoder Model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sJ8kkicmqTd"
      },
      "source": [
        "First, the encoder, a 2 layer `LSTM`.\n",
        "\n",
        "For a multi-layer LSTM, the input sentence, $X$, after being embedded goes into the first (bottom) layer of the LSTM.Along with input sequence , LSTM take in hidden state and cell state from previous time step and return new hidden state and cell state at each time step.Initial hidden state and cell state will be initialized to tensor of all zeros.Only hidden state from the first layer is passed as input to the second layer, and not the cell state.We will also output a context vector per layer, $z^l$.Our context vector will be both the final hidden state and the final cell state, i.e. $z^l = (h_T^l, c_T^l)$.\n",
        "\n",
        "Thus, representing each layer with a superscript, the hidden states in each layer are given by:\n",
        "\n",
        "$$\\begin{align*}\n",
        "(h_t^1, c_t^1) = \\text{EncoderLSTM}^1(e(x_t), (h_{t-1}^1, c_{t-1}^1))\\\\\n",
        "(h_t^2, c_t^2) = \\text{EncoderLSTM}^2(h_t^1, (h_{t-1}^2, c_{t-1}^2))\n",
        "\\end{align*}$$\n",
        "We create this in code by making an `Encoder` module, which inherit from `torch.nn.Module` and use the `super().__init__()` as some boilerplate code. The encoder takes the following arguments:\n",
        "\n",
        "*  `input_dim` is the size/dimensionality of the one-hot vectors that will be input to the encoder. This is equal to the input (source) vocabulary size.\n",
        "\n",
        "*  `emb_dim` is the dimensionality of the embedding layer. This layer converts the one-hot vectors into dense vectors with emb_dim dimensions.\n",
        "*   `hid_dim` is the dimensionality of the hidden and cell states.\n",
        "*   `n_layers` is the number of layers in the LSTM.\n",
        "* `dropout` is the amount of dropout to use. This is a regularization parameter to prevent overfitting.\n",
        "\n",
        "\n",
        "In the `forward` method, we pass in the source sentence, $X$, which is converted into dense vectors using the embedding layer, and then dropout is applied. These embeddings are then passed into the LSTM. As we pass a whole sequence to the LSTM, it will automatically do the recurrent calculation of the hidden states over the whole sequence.We do not pass an initial hidden or cell state to the LSTM. This is because, if no hidden/cell state is passed to the LSTM, it will automatically create an initial hidden/cell state as a tensor of all zeros.\n",
        "\n",
        "The `LSTM` returns: `outputs` (the top-layer hidden state for each time-step), hidden (the final hidden state for each layer, $h_T$, stacked on top of each other) and `cell` (the final cell state for each layer, $c_T$, stacked on top of each other).\n",
        "\n",
        "As we only need the final hidden and cell states (to make our context vector), `forward` only returns hidden and cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddiTgU8hLFj1"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        # Number of layers in the lstm\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "\n",
        "        \n",
        "\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "\n",
        "        # Regularization parameter\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "  \n",
        "    def forward(self, src):\n",
        "        # print(f'En-shape of src{src.shape}')#[8, 32]\n",
        " \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # print(f'En-shape of embedded{embedded.shape}')#[8, 32, 256]\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        # print(f'En-shape of outputs{outputs.shape}')#[8, 32, 512]\n",
        "        # print(f'En-shape of hidden{hidden.shape}')#[2, 32, 512]\n",
        "\n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2EObTGNmNxH"
      },
      "source": [
        "#### Decoder Model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j2iaJ43msw8"
      },
      "source": [
        "Next, we'll build our `decoder`, which will also be a 2-layer `LSTM`.\n",
        "\n",
        "The Decoder class does a single step of decoding, i.e. it ouputs single token per time-step. The first layer will receive a hidden and cell state from the previous time-step, $(s_{t-1}^1, c_{t-1}^1)$, and feeds it through the `LSTM` with the current embedded token, $y_t$, to produce a new hidden and cell state, $(s_t^1, c_t^1)$. The subsequent layers will use the hidden state from the layer below, $s_t^{l-1}$, and the previous hidden and cell states from their layer, $(s_{t-1}^l, c_{t-1}^l)$. This provides equations very similar to those in the encoder.\n",
        "\n",
        "$$\\begin{align*}\n",
        "(s_t^1, c_t^1) = \\text{DecoderLSTM}^1(d(y_t), (s_{t-1}^1, c_{t-1}^1))\\\\\n",
        "(s_t^2, c_t^2) = \\text{DecoderLSTM}^2(s_t^1, (s_{t-1}^2, c_{t-1}^2))\n",
        "\\end{align*}$$\n",
        "Remember that the initial hidden and cell states to our decoder are our context vectors, which are the final hidden and cell states of our encoder from the same layer, i.e. $(s_0^l,c_0^l)=z^l=(h_T^l,c_T^l)$.\n",
        "\n",
        "We then pass the hidden state from the top layer of the RNN, $s_t^L$, through a linear layer, $f$, to make a prediction of what the next token in the target (output) sequence should be, $\\hat{y}_{t+1}$.\n",
        "\n",
        "$$\\hat{y}_{t+1} = f(s_t^L)$$\n",
        "The arguments and initialization are similar to the Encoder class, except we now have an output_dim which is the size of the vocabulary for the output/target. There is also the addition of the Linear layer, used to make the predictions from the top layer hidden state.\n",
        "\n",
        "Within the forward method, we accept a batch of input tokens, previous hidden states and previous cell states. As we are only decoding one token at a time, the input tokens will always have a sequence length of 1. We unsqueeze the input tokens to add a sentence length dimension of 1. Then, similar to the encoder, we pass through an embedding layer and apply dropout. This batch of embedded tokens is then passed into the RNN with the previous hidden and cell states. This produces an output (hidden state from the top layer of the RNN), a new hidden state (one for each layer, stacked on top of each other) and a new cell state (also one per layer, stacked on top of each other). We then pass the output (after getting rid of the sentence length dimension) through the linear layer to receive our prediction. We then return the prediction, the new hidden state and the new cell state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKKhHPD2LHX1"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        # Number of layers in the lstm\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        # Regularization parameter\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    # Shape of input [batch_size]  \n",
        "    def forward(self, input, hidden, cell):\n",
        "        # print(f'De-shape of input{input.shape}')#[32]\n",
        "\n",
        "        \n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n directions in the decoder will always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        # print(f'De-shape of input{input.shape}')#[1, 32]\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # print(f'De-shape of embedded{embedded.shape}') #[1,32,256]   \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        # print(f'De-shape of output{output.shape}')#[1,32,512]\n",
        "        # print(f'De-shape of hidden{hidden.shape}')#[2,32,512]\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "        \n",
        "  \n",
        "        prediction = self.fc_out(output.squeeze(0))# [32, 1271]\n",
        "        # print(f'De-shape of prediction{prediction.shape}')\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9onop7QXuyTb"
      },
      "source": [
        "#### Seq2Seq (Encoder + Decoder) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKuv2NLvm2qd"
      },
      "source": [
        "For the final part of the implemenetation, we'll implement the `seq2seq` model. This will handle:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   receiving the input/source sentence\n",
        "*   using the encoder to produce the context vectors\n",
        "*   using the decoder to produce the predicted output/target sentence\n",
        "\n",
        "\n",
        "\n",
        "The `Seq2Seq` model takes in an Encoder, Decoder, and a device (used to place tensors on the GPU, if it exists).\n",
        "\n",
        "\n",
        "\n",
        "Our `forward` method takes the source sentence, target sentence and a teacher-forcing ratio. The teacher forcing ratio is used when training our model. When decoding, at each time-step we will predict what the next token in the target sequence will be from the previous tokens decoded, \n",
        "$\\hat{y}_{t+1}=f(s_t^L)$. With probability equal to the teaching forcing ratio (`teacher_forcing_ratio`) we will use the actual ground-truth next token in the sequence as the input to the decoder during the next time-step. However, with probability 1 - `teacher_forcing_ratio`, we will use the token that the model predicted as the next input to the model, even if it doesn't match the actual next token in the sequence.\n",
        "\n",
        "The first thing we do in the forward method is to create an outputs tensor that will store all of our predictions, $\\hat{Y}$.\n",
        "\n",
        "\n",
        "We then feed the input/source sentence, src, into the encoder and receive out final hidden and cell states.\n",
        "\n",
        "\n",
        "The first input to the decoder is the start of sequence (`<sos>`) token. As our trg tensor already has the `<sos>` token appended we get our $y_1$ by slicing into it. We know how long our target sentences should be (max_len), so we loop that many times. The last token input into the decoder is the one before the <eos> token - the\n",
        "\n",
        "During each iteration of the loop, we:\n",
        "\n",
        "pass the input, previous hidden and previous cell states ($y_t, s_{t-1}, c_{t-1}$) into the decoder\n",
        "receive a prediction, next hidden state and next cell state ($\\hat{y}_{t+1}, s_{t}, c_{t}$) from the decoder\n",
        "place our prediction, $\\hat{y}_{t+1}$/output in our tensor of predictions, $\\hat{Y}$/outputs\n",
        "decide if we are going to \"teacher force\" or not\n",
        "if we do, the next input is the ground-truth next token in the sequence, $y_{t+1}$/trg[t]\n",
        "if we don't, the next input is the predicted next token in the sequence, $\\hat{y}_{t+1}$/top1, which we get by doing an argmax over the output tensor\n",
        "Once we've made all of our predictions, we return our tensor full of predictions, $\\hat{Y}$/outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIf8EPxVLJhx"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        \n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEdRkOATm8Pn"
      },
      "source": [
        "## **Training Model**\n",
        "\n",
        "We define the encoder, decoder and then our Seq2Seq model, which we place on the device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i0_86JVLL-9"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyN8xipOnJEe"
      },
      "source": [
        "\n",
        "Next step is initializing the weights of our model.We initialize all weights from a uniform distribution between -0.08 and +0.08. For each module we loop through all of the parameters and sample them from a uniform distribution with `nn.init.uniform`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBhX5dKuLNar",
        "outputId": "e2b6e7d0-5e93-41b1-cd2c-2641a660afcc"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(2011, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(1271, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=512, out_features=1271, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUQsgXl2nTjl"
      },
      "source": [
        "\n",
        "We also define a function that will calculate the number of trainable parameters in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSdevJJNLPOt",
        "outputId": "645d40e0-7365-4236-b12c-a7af009c4071"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 8,848,631 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OOUhzaHnWZO"
      },
      "source": [
        "\n",
        "We define our optimizer, which we use to update our parameters in the training loop. Here, we'll use Adam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-CvhZwYLQoT"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu_jhImDnkdv"
      },
      "source": [
        "Next, we define our loss function as CrossEntropyLoss function. Our loss function calculates the average loss per token, however by passing the index of the <pad> token as the ignore_index argument we ignore the loss whenever the target token is a padding token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO11j3WELR5P"
      },
      "source": [
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tjK499bnq_d"
      },
      "source": [
        "\n",
        "Next, we'll define our training loop.\n",
        "\n",
        "First, we'll set the model into \"training mode\" with model and then iterate through our data iterator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-HReR1sLS8w"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        Question = batch.Question\n",
        "        Answer = batch.Answer\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(Question, Answer)\n",
        "        \n",
        "        #Answer = [Answer len, batch size]\n",
        "        #output = [Answer len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        Answer = Answer[1:].view(-1)\n",
        "        \n",
        "        #Answer = [(Answer len - 1) * batch size]\n",
        "        #output = [(Answer len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, Answer)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW8gt9gFnwHd"
      },
      "source": [
        "Next, we'll define our evaluation loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfm7iiOmLUhv"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            Question = batch.Question\n",
        "            Answer = batch.Answer\n",
        "\n",
        "            output = model(Question, Answer, 0) #turn off teacher forcing\n",
        "\n",
        "            #Answer = [Answer len, batch size]\n",
        "            #output = [Answer len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            Answer = Answer[1:].view(-1)\n",
        "\n",
        "            #Answer = [(Answer len - 1) * batch size]\n",
        "            #output = [(Answer len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, Answer)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJPV7sAwnzHO"
      },
      "source": [
        "\n",
        "Next, we'll create a function that we'll use to tell us how long an epoch takes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXrLres2LWHg"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUcmGUAhn1ik"
      },
      "source": [
        "We can finally start training our model!\n",
        "\n",
        "At each epoch, we'll be checking if our model has achieved the best validation loss so far. If it has, we'll update our best validation loss and save the parameters of our model . Then, when we come to test our model, we'll use the saved parameters used to achieve the best validation loss.\n",
        "\n",
        "We'll be printing out both the loss and the perplexity at each epoch. It is easier to see a change in perplexity than a change in loss as the numbers are much bigger."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGdzElxhLXKm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed1bc5f4-6b6c-4684-b708-6ac59a55fd06"
      },
      "source": [
        "N_EPOCHS = 30\n",
        "CLIP = 1\n",
        "train_los=[]\n",
        "test_los=[]\n",
        "best_test_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    train_los.append(train_loss)\n",
        "    test_loss = evaluate(model, test_iterator, criterion)\n",
        "    test_los.append(test_loss)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if test_loss < best_test_loss:\n",
        "        best_test_loss = test_loss\n",
        "        torch.save(model.state_dict(), 'saved-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Test. Loss: {test_loss:.3f} |  Test. PPL: {math.exp(test_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 3s\n",
            "\tTrain Loss: 5.568 | Train PPL: 261.915\n",
            "\t Test. Loss: 3.699 |  Test. PPL:  40.390\n",
            "Epoch: 02 | Time: 0m 3s\n",
            "\tTrain Loss: 4.425 | Train PPL:  83.518\n",
            "\t Test. Loss: 3.640 |  Test. PPL:  38.108\n",
            "Epoch: 03 | Time: 0m 3s\n",
            "\tTrain Loss: 4.353 | Train PPL:  77.724\n",
            "\t Test. Loss: 3.637 |  Test. PPL:  37.984\n",
            "Epoch: 04 | Time: 0m 3s\n",
            "\tTrain Loss: 4.336 | Train PPL:  76.403\n",
            "\t Test. Loss: 3.602 |  Test. PPL:  36.662\n",
            "Epoch: 05 | Time: 0m 3s\n",
            "\tTrain Loss: 4.304 | Train PPL:  73.994\n",
            "\t Test. Loss: 3.601 |  Test. PPL:  36.620\n",
            "Epoch: 06 | Time: 0m 3s\n",
            "\tTrain Loss: 4.282 | Train PPL:  72.415\n",
            "\t Test. Loss: 3.571 |  Test. PPL:  35.564\n",
            "Epoch: 07 | Time: 0m 3s\n",
            "\tTrain Loss: 4.213 | Train PPL:  67.542\n",
            "\t Test. Loss: 3.522 |  Test. PPL:  33.864\n",
            "Epoch: 08 | Time: 0m 3s\n",
            "\tTrain Loss: 4.151 | Train PPL:  63.503\n",
            "\t Test. Loss: 3.429 |  Test. PPL:  30.859\n",
            "Epoch: 09 | Time: 0m 3s\n",
            "\tTrain Loss: 4.104 | Train PPL:  60.577\n",
            "\t Test. Loss: 3.446 |  Test. PPL:  31.389\n",
            "Epoch: 10 | Time: 0m 3s\n",
            "\tTrain Loss: 4.079 | Train PPL:  59.100\n",
            "\t Test. Loss: 3.436 |  Test. PPL:  31.077\n",
            "Epoch: 11 | Time: 0m 3s\n",
            "\tTrain Loss: 4.030 | Train PPL:  56.238\n",
            "\t Test. Loss: 3.400 |  Test. PPL:  29.958\n",
            "Epoch: 12 | Time: 0m 3s\n",
            "\tTrain Loss: 4.001 | Train PPL:  54.674\n",
            "\t Test. Loss: 3.393 |  Test. PPL:  29.760\n",
            "Epoch: 13 | Time: 0m 3s\n",
            "\tTrain Loss: 3.971 | Train PPL:  53.014\n",
            "\t Test. Loss: 3.395 |  Test. PPL:  29.807\n",
            "Epoch: 14 | Time: 0m 3s\n",
            "\tTrain Loss: 3.941 | Train PPL:  51.487\n",
            "\t Test. Loss: 3.394 |  Test. PPL:  29.789\n",
            "Epoch: 15 | Time: 0m 3s\n",
            "\tTrain Loss: 3.926 | Train PPL:  50.701\n",
            "\t Test. Loss: 3.397 |  Test. PPL:  29.862\n",
            "Epoch: 16 | Time: 0m 3s\n",
            "\tTrain Loss: 3.896 | Train PPL:  49.229\n",
            "\t Test. Loss: 3.422 |  Test. PPL:  30.637\n",
            "Epoch: 17 | Time: 0m 3s\n",
            "\tTrain Loss: 3.863 | Train PPL:  47.625\n",
            "\t Test. Loss: 3.383 |  Test. PPL:  29.444\n",
            "Epoch: 18 | Time: 0m 3s\n",
            "\tTrain Loss: 3.840 | Train PPL:  46.532\n",
            "\t Test. Loss: 3.380 |  Test. PPL:  29.381\n",
            "Epoch: 19 | Time: 0m 3s\n",
            "\tTrain Loss: 3.827 | Train PPL:  45.929\n",
            "\t Test. Loss: 3.400 |  Test. PPL:  29.972\n",
            "Epoch: 20 | Time: 0m 3s\n",
            "\tTrain Loss: 3.799 | Train PPL:  44.672\n",
            "\t Test. Loss: 3.399 |  Test. PPL:  29.939\n",
            "Epoch: 21 | Time: 0m 3s\n",
            "\tTrain Loss: 3.778 | Train PPL:  43.727\n",
            "\t Test. Loss: 3.381 |  Test. PPL:  29.414\n",
            "Epoch: 22 | Time: 0m 3s\n",
            "\tTrain Loss: 3.761 | Train PPL:  42.985\n",
            "\t Test. Loss: 3.402 |  Test. PPL:  30.026\n",
            "Epoch: 23 | Time: 0m 3s\n",
            "\tTrain Loss: 3.744 | Train PPL:  42.271\n",
            "\t Test. Loss: 3.400 |  Test. PPL:  29.972\n",
            "Epoch: 24 | Time: 0m 3s\n",
            "\tTrain Loss: 3.711 | Train PPL:  40.902\n",
            "\t Test. Loss: 3.356 |  Test. PPL:  28.684\n",
            "Epoch: 25 | Time: 0m 3s\n",
            "\tTrain Loss: 3.704 | Train PPL:  40.611\n",
            "\t Test. Loss: 3.393 |  Test. PPL:  29.758\n",
            "Epoch: 26 | Time: 0m 3s\n",
            "\tTrain Loss: 3.662 | Train PPL:  38.958\n",
            "\t Test. Loss: 3.378 |  Test. PPL:  29.315\n",
            "Epoch: 27 | Time: 0m 3s\n",
            "\tTrain Loss: 3.656 | Train PPL:  38.707\n",
            "\t Test. Loss: 3.364 |  Test. PPL:  28.897\n",
            "Epoch: 28 | Time: 0m 3s\n",
            "\tTrain Loss: 3.617 | Train PPL:  37.225\n",
            "\t Test. Loss: 3.378 |  Test. PPL:  29.321\n",
            "Epoch: 29 | Time: 0m 3s\n",
            "\tTrain Loss: 3.586 | Train PPL:  36.088\n",
            "\t Test. Loss: 3.364 |  Test. PPL:  28.898\n",
            "Epoch: 30 | Time: 0m 3s\n",
            "\tTrain Loss: 3.583 | Train PPL:  35.990\n",
            "\t Test. Loss: 3.363 |  Test. PPL:  28.881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ug0Vuj26mWd"
      },
      "source": [
        "Let's view train and test loss of our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjUCB1yYMfIa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "bac0473a-e19b-4fc6-9023-71d28418c79d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-darkgrid')\n",
        "plt.figure()\n",
        "plt.plot(train_los, color = 'magenta')\n",
        "plt.plot(test_los, color = '#606060')\n",
        "plt.title('Train and test Loss')\n",
        "plt.legend(['train_loss', 'test_loss'], loc = 'upper right')\n",
        "plt.grid(axis = 'y', c = 'black', alpha = 0.2)\n",
        "plt.grid(axis = 'x', c = 'black', alpha = 0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_oFL7fGcfIV"
      },
      "source": [
        "## **Inference**\n",
        "\n",
        "\n",
        "Now, we'll grab some answers to questions from our dataset and see how well our model did."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwfTn-eauXf9"
      },
      "source": [
        "def predict_answer(sentence, src_field, trg_field, model, device, max_len = 50):\n",
        "\n",
        "    model.eval()\n",
        "        \n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('en_core_web_sm')\n",
        "        #('en_core_web_sm')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    \n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "    # src_len = torch.LongTensor([len(src_indexes)])\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        hidden,cell = model.encoder(src_tensor)\n",
        "\n",
        "        \n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    # attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\n",
        "    \n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
        "            # output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
        "\n",
        "            \n",
        "        pred_token = output.argmax(1).item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0fPT8HC1OeR",
        "outputId": "518fe0af-73e8-4b1c-bc9e-9d89b7ba2a04"
      },
      "source": [
        "example_idx = 20\n",
        "\n",
        "src = vars(train_data.examples[example_idx])['Question']\n",
        "trg = vars(train_data.examples[example_idx])['Answer']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['appreciated', 'during', 'his', 'lifetime', ',', 'did', 'his', 'fame', 'grow', 'in', 'the', 'years', 'after', 'his', 'death', '?']\n",
            "trg = ['yes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oloJN1eW1lVD",
        "outputId": "964df63d-7c0c-4c48-90e8-c404fc85b0e7"
      },
      "source": [
        "pred_answer= predict_answer(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = {pred_answer}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = ['yes', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwxmb2dx14wp"
      },
      "source": [
        "Let's predict some answers from test set too"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRm3_PjC12TC",
        "outputId": "314b9708-6a9c-45be-d544-5eeb5f51ea53"
      },
      "source": [
        "\n",
        "example_idx = 6\n",
        "\n",
        "src = vars(test_data.examples[example_idx])['Question']\n",
        "trg = vars(test_data.examples[example_idx])['Answer']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['was', 'faraday', 'a', 'devout', 'christian', '?']\n",
            "trg = ['yes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqgDGW7i23zZ",
        "outputId": "072fb079-2efd-49f8-8e94-4dfb603bd7a7"
      },
      "source": [
        "pred_answer= predict_answer(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = {pred_answer}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = ['yes', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVrVzGgW3xHN",
        "outputId": "a773abc2-08ae-42bd-dcba-a0a3802cb35c"
      },
      "source": [
        "example_idx = 30\n",
        "\n",
        "src = vars(test_data.examples[example_idx])['Question']\n",
        "trg = vars(test_data.examples[example_idx])['Answer']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['do', 'lobsters', 'feel', 'pain', '?']\n",
            "trg = ['yes', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCwRcvFt30hQ",
        "outputId": "75b96c5c-a503-4360-b521-bef31b674845"
      },
      "source": [
        "pred_answer= predict_answer(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = {pred_answer}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = ['yes', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-4GMNR236tx",
        "outputId": "bdf3df13-6396-48af-ba96-98b3e2a5eb85"
      },
      "source": [
        "example_idx = 16\n",
        "\n",
        "src = vars(test_data.examples[example_idx])['Question']\n",
        "trg = vars(test_data.examples[example_idx])['Answer']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['are', 'turtles', 'being', 'examined', 'for', 'longevity', 'genes', '?']\n",
            "trg = ['yes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bcg7UtBv3-o8",
        "outputId": "b00f4e29-1c2d-45da-cb1d-57fd7bb8e1fc"
      },
      "source": [
        "pred_answer= predict_answer(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = {pred_answer}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = ['yes', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkJ9Yg9o3__I",
        "outputId": "7d3a763c-c595-47bb-af3c-7bf13224b202"
      },
      "source": [
        "example_idx = 102\n",
        "\n",
        "src = vars(test_data.examples[example_idx])['Question']\n",
        "trg = vars(test_data.examples[example_idx])['Answer']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['what', 'is', 'the', 'primary', 'item', 'in', 'an', 'otter', \"'s\", 'diet', '?']\n",
            "trg = ['fish', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neFw6bpM4LEs",
        "outputId": "c30d3ed6-d1ec-42b4-a204-1c9e4600471f"
      },
      "source": [
        "pred_answer= predict_answer(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = {pred_answer}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = ['<unk>', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IgtwrZx5uPn",
        "outputId": "99f34fa1-2dfc-4bcc-d3ed-3e2a4d38c238"
      },
      "source": [
        "example_idx = 300\n",
        "\n",
        "src = vars(test_data.examples[example_idx])['Question']\n",
        "trg = vars(test_data.examples[example_idx])['Answer']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['was', 'roosevelt', \"'s\", 'family', 'rich', '?']\n",
            "trg = ['yes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MqdeGKk5vzx",
        "outputId": "f92c161a-5c30-462f-a44d-f8168939d330"
      },
      "source": [
        "pred_answer= predict_answer(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = {pred_answer}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = ['no', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}